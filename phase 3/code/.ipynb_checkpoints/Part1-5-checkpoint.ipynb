{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "import logging\n",
    "\n",
    "for _ in (\"boto\", \"elasticsearch\", \"urllib3\"):\n",
    "    logging.getLogger(_).setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(starting_URLs, max_document=2000):\n",
    "    \n",
    "    frontier_urls = list()\n",
    "    doc_count = [-1]\n",
    "    data = {}\n",
    "    data['articles'] = []\n",
    "    \n",
    "    class ArticleSpider(scrapy.Spider):\n",
    "        name = \"article\"\n",
    "        start_urls = starting_URLs\n",
    "        \n",
    "        def parse(self, response):\n",
    "            string = response.css(\"#paper-header > pre::text\").get().splitlines()\n",
    "            current_url = response.css(\"head > link:nth-child(3)\").attrib['href']\n",
    "            current_ref = list()\n",
    "            \n",
    "            for i in range(1, 11):\n",
    "                CSS_selector = \"#references > div.card-content > div > div.citation-list__citations > div:nth-child(\"+str(i)+\") > div.citation__body > h2 > a\"\n",
    "                rel_url = response.css(CSS_selector)\n",
    "                \n",
    "                if rel_url != []:\n",
    "                    if \"href\" in rel_url.attrib:\n",
    "                        \n",
    "                        rel_url = response.css(CSS_selector).attrib['href']\n",
    "                        next_url = response.urljoin(rel_url) \n",
    "                        current_ref.append(next_url[-next_url[::-1].find('/'):])\n",
    "\n",
    "                        if next_url not in frontier_urls:\n",
    "                            frontier_urls.append(next_url)      \n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            correct_date = response.css(\"#paper-header > div.flex-row.paper-meta > li:nth-child(2) > span > span > span > span::text\").get()\n",
    "            if correct_date is None:\n",
    "                correct_date = ''\n",
    "            else:\n",
    "                if correct_date.isdigit() == False:\n",
    "                    correct_date = response.css(\"#paper-header > div.flex-row.paper-meta > li:nth-child(2) > span > span:nth-child(2) > span > span::text\").get()\n",
    "            \n",
    "            correct_authors = re.search(\"\\{.*\\}\", string[2]).group(0)[1:-1]\n",
    "                                       \n",
    "            if correct_authors is not None:\n",
    "                correct_authors = re.split(' and ', correct_authors)\n",
    "            else:\n",
    "                correct_authors = \"\"\n",
    "                \n",
    "            data['articles'].append({\n",
    "                'id': current_url[-current_url[::-1].find('/'):],                                                                                                   \n",
    "                'title': response.css(\"#paper-header > h1::text\").get(),\n",
    "                'authors': correct_authors,\n",
    "                'date': correct_date,\n",
    "                'abstract': response.css(\"head > meta:nth-child(7)\").attrib['content'],\n",
    "                'references': current_ref\n",
    "            })\n",
    "            \n",
    "            doc_count[0] += 1\n",
    "            \n",
    "            if doc_count[0] >= max_document-len(starting_URLs):\n",
    "                process.stop()\n",
    "                \n",
    "            yield scrapy.Request(frontier_urls[doc_count[0]], callback=self.parse)\n",
    "    \n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(ArticleSpider)\n",
    "    process.start()\n",
    "    \n",
    "    file = open(\"papers_index.json\", 'w')\n",
    "    string = json.dumps(data, indent=4)\n",
    "    file.write(string)\n",
    "    file.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Insert & Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_elasticSearch(json_data, host, port):\n",
    "\n",
    "    es = Elasticsearch([{'host': host, 'port': int(port)}])\n",
    "    \n",
    "    for i in range(len(json_data)):\n",
    "        es.index(index='paper_index',doc_type='paper',id=i,body={\"paper\": json_data[i]})\n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_elasticSearch(es):\n",
    "    leng = es.count(index='paper_index', doc_type='paper')[\"count\"]\n",
    "    for i in range(leng):\n",
    "        es.delete(index='paper_index',doc_type='paper',id=i)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(es, alpha):\n",
    "    \n",
    "    N = es.count(index='paper_index', doc_type='paper')[\"count\"]\n",
    "    total_docs = list()\n",
    "    \n",
    "    for i in range(N):\n",
    "        doc_info = es.get(index='paper_index', doc_type='paper', id=i)\n",
    "        total_docs.append(doc_info['_source']['paper']['id'])\n",
    "        \n",
    "    map_id = dict(zip(total_docs, range(N)))\n",
    "    \n",
    "    P = np.zeros((N, N), dtype=float)\n",
    "    \n",
    "    for i in range(N):\n",
    "        doc_ref = es.get(index='paper_index', doc_type='paper', id=i)['_source']['paper']['references']\n",
    "        for j in range(len(doc_ref)):\n",
    "            if doc_ref[j] in total_docs:\n",
    "                P[i][map_id[doc_ref[j]]] += 1\n",
    "                \n",
    "    P /= N\n",
    "    P[np.where(~P.any(axis=1))[0]] = np.ones(N, dtype=float) / N\n",
    "    P = (1 - alpha) * P + alpha * (np.ones((N, N), dtype=float) / N)\n",
    "\n",
    "    nex_v = np.ones(N, dtype=float) / N\n",
    "    v = np.zeros(N, dtype=float)\n",
    "    \n",
    "    while(np.sqrt(np.sum(np.power(nex_v - v, 2))) > 1e-6):\n",
    "        v = nex_v\n",
    "        nex_v = np.matmul(v, P)\n",
    "        \n",
    "    for i in range(N):\n",
    "        es.update(index='paper_index', doc_type='paper', id=i, body={\n",
    "            'doc' : {\"paper\" : {'pageRank' : v[i]}}\n",
    "        })\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(es, title, w_title, date, w_date, abstract, w_abstract, inv_pageRank=True):\n",
    "    if inv_pageRank == True:\n",
    "        res= es.search(index='paper_index', doc_type='paper', body={\n",
    "            \"query\": {\n",
    "                \"function_score\": {\n",
    "                  \"functions\": [\n",
    "                      {\n",
    "                        \"field_value_factor\" : {\n",
    "                                \"field\" : \"paper.pageRank\",\n",
    "                                \"factor\" : 1e9,\n",
    "                                \"modifier\": \"log1p\"\n",
    "                       }\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match\": { \"paper.title\": { \n",
    "                                                      \"query\": title,\n",
    "                                                      \"operator\": \"or\",\n",
    "                                                      \"fuzziness\": 1} } },\n",
    "                          \"weight\": w_title\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match_phrase\": { \"paper.title\": title } },\n",
    "                          \"weight\": w_title\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match\": { \"paper.abstract\": { \n",
    "                                                      \"query\": abstract,\n",
    "                                                      \"operator\": \"or\",\n",
    "                                                      \"fuzziness\": 1} } },\n",
    "                          \"weight\": w_abstract\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match_phrase\": { \"paper.abstract\": abstract } },\n",
    "                          \"weight\": w_abstract\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": {\"range\":{\"paper.date\":{\"gt\": str(date-1)}}},\n",
    "                          \"weight\": w_date\n",
    "                      },\n",
    "                  ],\n",
    "                  \"score_mode\": \"sum\",\n",
    "                  \"boost_mode\": \"multiply\",\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        res= es.search(index='paper_index', doc_type='paper', body={\n",
    "            \"query\": {\n",
    "                \"function_score\": {\n",
    "                  \"functions\": [\n",
    "                      {\n",
    "                          \"filter\": { \"match\": { \"paper.title\": { \n",
    "                                                      \"query\": title,\n",
    "                                                      \"operator\": \"or\",\n",
    "                                                      \"fuzziness\": 1} } },\n",
    "                          \"weight\": w_title\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match_phrase\": { \"paper.title\": title } },\n",
    "                          \"weight\": w_title\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match\": { \"paper.abstract\": { \n",
    "                                                      \"query\": abstract,\n",
    "                                                      \"operator\": \"or\",\n",
    "                                                      \"fuzziness\": 1} } },\n",
    "                          \"weight\": w_abstract\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": { \"match_phrase\": { \"paper.abstract\": abstract } },\n",
    "                          \"weight\": w_abstract\n",
    "                      },\n",
    "                      {\n",
    "                          \"filter\": {\"range\":{\"paper.date\":{\"gt\": str(date-1)}}},\n",
    "                          \"weight\": w_date\n",
    "                      },\n",
    "                  ],\n",
    "                  \"score_mode\": \"sum\",\n",
    "                  \"boost_mode\": \"multiply\",\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    rank = 1\n",
    "    for hit in res['hits']['hits']:\n",
    "        print(str(rank)+\". title:\", hit['_source']['paper']['title'])\n",
    "        print(\"abstract:\", hit['_source']['paper']['abstract'])\n",
    "        print(\"authors:\", hit['_source']['paper']['authors'])\n",
    "        print(\"date:\", hit['_source']['paper']['date'])\n",
    "        print ()\n",
    "        rank += 1\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HITS(es, n):\n",
    "    all_authors, all_papers = list(), list()\n",
    "    N = es.count(index='paper_index', doc_type='paper')[\"count\"]\n",
    "    \n",
    "    for i in range(N):\n",
    "        cur_doc = es.get(index='paper_index', doc_type='paper', id=i)['_source']['paper']\n",
    "        all_papers.append(cur_doc['id'])\n",
    "        for author in cur_doc['authors']:\n",
    "            all_authors.append(author.lower())\n",
    "            \n",
    "    map_id = dict(zip(all_papers, range(N)))\n",
    "    all_authors = np.unique(all_authors)\n",
    "    M = len(all_authors)\n",
    "    map_author = dict(zip(all_authors, range(M)))\n",
    "    \n",
    "    A = np.zeros((M, M), dtype=float)\n",
    "    \n",
    "    for i in range(N):\n",
    "        cur_doc = es.get(index='paper_index', doc_type='paper', id=i)['_source']['paper']\n",
    "        for ref_doc in cur_doc['references']:\n",
    "            if ref_doc in list(map_id):\n",
    "                for cur_author in cur_doc['authors']:\n",
    "                    des_authors = es.get(index='paper_index', doc_type='paper', id=map_id[ref_doc])['_source']['paper']['authors']\n",
    "                    for author in des_authors:\n",
    "                        A[map_author[cur_author.lower()]][map_author[author.lower()]] += 1\n",
    "                        \n",
    "    h = np.ones(M, dtype=float)\n",
    "    a = np.ones(M, dtype=float)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        for i in range(M):\n",
    "            h[i] = np.sum(a[(A[i] != 0)])\n",
    "            a[i] = np.sum(h[(A[:,i] != 0)])\n",
    "        \n",
    "        h /= np.sum(h)\n",
    "        a /= np.sum(a)\n",
    "        \n",
    "    authority_list = dict(zip(list(map_author), list(a)))\n",
    "    return sorted(authority_list.items(), key=lambda item: item[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console: User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menu():\n",
    "    print(\"1. Crawl semanticScholar.com (Only Once run this command!)\\n2. Insert data to ElasticSearch\\n3. Delete data from ElasticSearch\\n4. Calculate pageRank\\n5. Search\\n6. HITS\\n7. Exit\")\n",
    "    print(\"order of computing: 1 -> 2 -> 3 or 4 or 6 or 5\")\n",
    "    print(\"please enter your command by number (e.g. 1):\")\n",
    "    return\n",
    "\n",
    "documents, es, flag = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Crawl semanticScholar.com (Only Once run this command!)\n",
      "2. Insert data to ElasticSearch\n",
      "3. Delete data from ElasticSearch\n",
      "4. Calculate pageRank\n",
      "5. Search\n",
      "6. HITS\n",
      "7. Exit\n",
      "order of computing: 1 -> 2 -> 3 or 4 or 6 or 5\n",
      "please enter your command by number (e.g. 1):\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "menu()\n",
    "while True:\n",
    "    cmd = int(input())\n",
    "    if cmd == 1:\n",
    "        if flag == 1:\n",
    "            print(\"You've already crawled needed data!\")\n",
    "        else:\n",
    "            print(\"Please enter number of pages to crawl (e.g. 200):\")\n",
    "            page_num = int(input())\n",
    "            urls = [\n",
    "                \"https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf\",\n",
    "                \"https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\n",
    "                \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\"\n",
    "            ]\n",
    "            documents = crawler(urls, page_num)\n",
    "            flag = 1\n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 2:\n",
    "        print(\"Make sure that the elasticSearch server is running!\")\n",
    "        print(\"Host (e.g. localhost):\")\n",
    "        host = input()\n",
    "        print(\"Port (e.g. 9200):\")\n",
    "        port = input()\n",
    "        print(\"takes a few seconds...(ignore warnings!!)\")\n",
    "        if documents == 0:\n",
    "            print(\"ERROR, do command 1 first!\")\n",
    "        else:\n",
    "            es = insert_elasticSearch(documents['articles'], host, port)\n",
    "        \n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 3:\n",
    "        delete_elasticSearch(es)\n",
    "            \n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 4:\n",
    "        print(\"alpha (e.g. 0.1):\")\n",
    "        alpha = float(input())\n",
    "        print(\"Calculating...(ignore warnings!!)\")\n",
    "        page_rank(es, alpha)\n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 5:\n",
    "        print(\"title:\")\n",
    "        title = input()\n",
    "        print(\"weight:\")\n",
    "        w_t = int(input())\n",
    "        print(\"date:\")\n",
    "        date = int(input())\n",
    "        print(\"weight:\")\n",
    "        w_d = int(input())\n",
    "        print(\"abstract:\")\n",
    "        abstract = input()\n",
    "        print(\"weight:\")\n",
    "        w_a = int(input())\n",
    "        print(\"Involve pageRank (1 or 0):\")\n",
    "        pr = int(input())\n",
    "        if pr == 1:\n",
    "            pr = True\n",
    "        else:\n",
    "            pr = False\n",
    "        print(\"List of top 10 papers by order:\")\n",
    "        search(es, title, w_t, date, w_d, abstract, w_a, inv_pageRank=pr)\n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 6:\n",
    "        \n",
    "        print(\"top N authors, select N:\")\n",
    "        n = int(input())\n",
    "        print(\"Calculating...(ignore warnings!!)\")\n",
    "        auth = HITS(es, n)\n",
    "        print(\"List of top \"+str(n)+\" authors:\")\n",
    "        for i in range(len(auth)):\n",
    "            print(i+1, auth[i][0])\n",
    "            \n",
    "        print()\n",
    "        menu()\n",
    "        \n",
    "    elif cmd == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
